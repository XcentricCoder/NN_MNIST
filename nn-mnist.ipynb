{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/digit-recognizer')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_dir = '/kaggle/input'\nfor root, dirs, files in os.walk(input_dir):\n    for dir_name in dirs:\n        print(f\"Directory: {dir_name}\")\n    for file_name in files:\n        print(f\"File: {file_name}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\nprint(train_df.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df=pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\nprint(test_df.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train_df.drop(columns=['label']).values\ny_train = train_df['label'].values\nX_test = test_df.values\n\n# Normalize pixel values\nX_train = X_train / 255.0\nX_test = X_test / 255.0\n\n# One-hot encode the labels\ndef one_hot_encode(labels, num_classes):\n    return np.eye(num_classes)[labels]\n\ny_train_encoded = one_hot_encode(y_train, 10)\nX_train, X_val, y_train_encoded, y_val_encoded = train_test_split(X_train, y_train_encoded, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"X_train shape: {X_train.shape}\")  # Should be (num_samples, num_features)\nprint(f\"y_train_encoded shape: {y_train_encoded.shape}\")  # Should be (num_samples, num_classes)\nnum_features = X_train.shape[1]\nnum_classes = y_train_encoded.shape[1]\n\nprint(f\"Number of features: {num_features}\")\nprint(f\"Number of classes: {num_classes}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn_architecture = [\n    {\"input_dim\": X_train.shape[1], \"output_dim\": 128, \"activation\": \"relu\"},\n    {\"input_dim\": 128, \"output_dim\": 64, \"activation\": \"relu\"},\n    {\"input_dim\": 64, \"output_dim\": 32, \"activation\": \"tanh\"},\n    {\"input_dim\": 32, \"output_dim\": 10, \"activation\": \"softmax\"},\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_layers(nn_architecture, seed=99):\n    np.random.seed(seed)\n    params_values = {}\n    \n    for idx, layer in enumerate(nn_architecture):\n        layer_idx = idx + 1\n        input_dim = layer[\"input_dim\"]\n        output_dim = layer[\"output_dim\"]\n        \n        params_values['W' + str(layer_idx)] = np.random.randn(output_dim, input_dim) * 0.1\n        params_values['b' + str(layer_idx)] = np.random.randn(output_dim, 1) * 0.1\n    \n    return params_values\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_values = init_layers(nn_architecture, seed=2)\nfor key, value in params_values.items():\n    print(f\"{key} shape: {value.shape}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sigmoid(z):\n    return 1/(1+np.exp(-z))\ndef relu(z):\n    return np.maximum(0,z)\ndef sigmoid_backwards(dA,z):\n    sig=sigmoid(z)\n    return dA*sig*(1-sig)\ndef relu_backwards(dA, z):\n    dZ = np.array(dA, copy=True)\n    dZ[z <= 0] = 0\n    return dZ\ndef linear(z):\n    return z\ndef linear_backwards(dA,z):\n    dZ=dA\n    return dA\ndef tanh(z):\n    return (2/(1+np.exp(-2*z)))-1\ndef tanh_backwards(dA,z):\n    y=tanh(z)\n    return 1-(y)^2\ndef softmax(z):\n    z_shifted=z-np.max(z,axis=0, keepdims=True)\n    exp_z=np.exp(z_shifted)\n    A=exp_z/(np.sum(exp_z,axis=0,keepdims=True))\n    return A\ndef softmax_backwards(dA, z):\n    A = softmax(z)\n    dZ = A - dA\n    return dZ\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def single_layer_forward_prop(A_prev, W_curr, B_curr, activation):\n    print(f\"A_prev shape: {A_prev.shape}\")\n    print(f\"W_curr shape: {W_curr.shape}\")\n    print(f\"B_curr shape: {B_curr.shape}\")\n    Z_curr = np.dot( A_prev,W_curr.T) + B_curr.T\n    \n    if activation == \"relu\":\n        activation_func = relu\n    elif activation == \"linear\":\n        activation_func = lambda x: x\n    elif activation == \"tanh\":\n        activation_func = tanh\n    elif activation == \"sigmoid\":\n        activation_func = sigmoid\n    elif activation == \"softmax\":\n        activation_func = softmax\n    else:\n        raise Exception(\"Non-supported activation function\")\n    \n    return activation_func(Z_curr), Z_curr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def full_forward_propagation(X, params_values, nn_architecture):\n    memory = {}\n    A_curr = X\n    \n    for idx, layer in enumerate(nn_architecture):\n        layer_idx = idx + 1\n        A_prev = A_curr\n        activ_func_curr = layer[\"activation\"]\n        W_curr = params_values[\"W\" + str(layer_idx)]\n        B_curr = params_values[\"b\" + str(layer_idx)]\n        A_curr, Z_curr = single_layer_forward_prop(A_prev, W_curr, B_curr, activ_func_curr)\n        memory[\"A\" + str(layer_idx)] = A_curr\n        memory[\"Z\" + str(layer_idx)] = Z_curr\n    \n    return A_curr, memory","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation):\n    m = A_prev.shape[1]\n    \n    if activation == \"relu\":\n        backward_activation_function = relu_backwards\n    elif activation == \"linear\":\n        backward_activation_function = lambda dA, Z: dA\n    elif activation == \"tanh\":\n        backward_activation_function = tanh_backwards\n    elif activation == \"sigmoid\":\n        backward_activation_function = sigmoid_backwards\n    elif activation == \"softmax\":\n        backward_activation_function = softmax_backwards\n    else:\n        raise Exception(\"Non-supported activation function\")\n    \n    dZ_curr = backward_activation_function(dA_curr, Z_curr)\n    dW_curr = np.dot(dZ_curr, A_prev) / A_prev.shape[1]\n    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / A_prev.shape[1]\n    dA_prev = np.dot(W_curr.T, dZ_curr)\n    \n    return dA_prev, dW_curr, db_curr\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n    grads_values = {}\n    m = Y.shape[1]\n    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat))\n    \n    for idx, layer in reversed(list(enumerate(nn_architecture))):\n        layer_idx = idx + 1\n        A_prev = memory[\"A\" + str(layer_idx - 1)]\n        Z_curr = memory[\"Z\" + str(layer_idx)]\n        W_curr = params_values[\"W\" + str(layer_idx)]\n        b_curr = params_values[\"b\" + str(layer_idx)]\n        \n        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n            dA_prev, W_curr, b_curr, Z_curr, A_prev, layer[\"activation\"]\n        )\n        \n        print(f\"Layer {layer_idx}: A_prev shape = {A_prev.shape}, W_curr shape = {W_curr.shape}\")\n        \n        grads_values[\"dW\" + str(layer_idx)] = dW_curr\n        grads_values[\"db\" + str(layer_idx)] = db_curr\n    \n    return grads_values\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def update(params_values, grads_values, nn_architecture, learning_rate):\n    for layer_idx, layer in enumerate(nn_architecture):\n        layer_idx += 1\n        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]\n        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n    \n    return params_values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_prob_into_class(probabilities):\n    return np.argmax(probabilities, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_one_hot_to_class(Y_one_hot):\n    return np.argmax(Y_one_hot, axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_cost_function(Y_hat,Y):\n    m=Y.shape[0]\n    cost=-(1/m)*(np.dot(Y,np.log(Y_hat))+np.dot(1-Y,np.log(1-Y_hat)))\n    return np.squeeze(cost) \ndef get_accuracy_value(Y_hat, Y):\n    Y_hat = convert_prob_into_class(Y_hat)\n    Y=convert_one_hot_to_class(Y)\n    return (Y_hat == Y).all(axis=0).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(X, Y, nn_architecture, epochs, learning_rate):\n    params_values = init_layers(nn_architecture, seed=2)\n    cost_history = []\n    accuracy_history = []\n    \n    for i in range(epochs):\n        Y_hat, memory = full_forward_propagation(X, params_values, nn_architecture)\n        cost = get_cost_function(Y_hat, Y)\n        cost_history.append(cost)\n        accuracy = get_accuracy_value(Y_hat, Y)\n        accuracy_history.append(accuracy)\n        \n        grads_values = full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture)\n        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n        \n        print(f\"Epoch {i + 1}/{epochs}, Cost: {cost}, Accuracy: {accuracy}\")\n    \n    return params_values, cost_history, accuracy_history\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 20\nlearning_rate = 0.01\nparams_values, cost_history, accuracy_history = train(X_train, y_train_encoded, nn_architecture, epochs, learning_rate)\n\n# Evaluate the model\nY_hat, _ = full_forward_propagation(X_test, params_values, nn_architecture)\ntest_accuracy = get_accuracy_value(Y_hat, np.argmax(y_test, axis=1))\nprint(f'Test Accuracy: {test_accuracy}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}